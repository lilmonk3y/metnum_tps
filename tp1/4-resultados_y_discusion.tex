\section{Resultados}
\label{resultados}

\subsection{Resultado Elim. Gaussiana sin pivoteo}
\label{resultados EG}

\subsubsection{Ejercicio 1.1}
El pseudocódigo de la función implementada se puede encontrar en la sección \ref{EG_sinP}

\subsubsection{Ejercicio 1.2}

Si tomamos de ejemplo a la siguiente matriz:

\begin{center}
$\begin{bmatrix}
0 & 1 & -1 & 3\\
-2 & -0.5 & 0 & 0\\
4 & 1 & -2 & 4\\
-6 & -1 & 2 & -3
\end{bmatrix}$
\end{center}
                  
Es esperado que al procesarla se produzca un error debido a que para resolver el sistema necesitamos intercambiar filas y columnas para llevar la matriz a una forma triangular superior y el algoritmo no lo realiza, pues solo se permite operaciones de multiplicación y suma para reducir los elementos debajo de la diagonal principal.

\subsection{Resultado Elim. Gaussiana con pivoteo}
\label{resultados EG c/p}
\subsubsection{Ejercicio 2.1}
El pseudocódigo de la función implementada se puede encontrar en la sección \ref{seccion_EG_pivot}

\subsubsection{Ejercicio 2.2}

Buscamos un ejemplo tal que la matriz de coeficientes sea singular, es decir, no tenga inversa. Esto ocurre cuando alguna fila de la matriz puede expresarse como combinación lineal de las otras filas. Por ejemplo, dada la siguiente matriz:

\begin{center}
$\begin{bmatrix}
1 & -1 & 3\\
-2 & 2 & -6\\
1 & 5 & 7
\end{bmatrix}$
\end{center}

La triangulación da como resultado la siguiente matriz:

\begin{center}
$\begin{bmatrix}
1 & 2 & 3\\
0 & 0 & 0\\
0 & 1 & 2
\end{bmatrix}$
\end{center}

Esta ecuación solo tiene resultado cuando b$_2$ es igual a 0. En ese caso tiene infinitas soluciones. Caso contrario, no existe solución.

Otro ejemplo, es tener una matriz tal que la primera columna tenga todos sus elementos iguales a 0. El problema aquí es que el primer elemento de la diagonal principal (posición [0,0]) es 0, lo que significa que no se puede usar como pivote.

\begin{center}
$\begin{bmatrix}
0 & 1 & -1 & 3\\
0 & -0.5 & 0 & 0\\
0 & 1 & -2 & 4\\
0 & -1 & 2 & -3
\end{bmatrix}$
\end{center}

\subsubsection{Ejercicio 2.3}

Se tiene el sistema de ecuaciones lineales con A y b igual a:

\[ \begin{bmatrix}
1 & 2+\epsilon & 3-\epsilon\\
1-\epsilon & 2 & 3+\epsilon\\
1+\epsilon & 2-\epsilon & 3
\end{bmatrix} ,
\begin{bmatrix}
6\\
6\\
6
\end{bmatrix}\]

Se calcula la matriz inversa de A, $A^{-1}$:
\begin{center}
$\begin{bmatrix}
\frac{\epsilon+1}{18\epsilon} & \frac{\epsilon-8}{18\epsilon} & \frac{\epsilon+7}{18\epsilon}\\
\frac{\epsilon+7}{18\epsilon} & \frac{\epsilon-2}{18\epsilon} & \frac{\epsilon-5}{18\epsilon}\\
\frac{\epsilon-5}{18\epsilon} & \frac{\epsilon+4}{18\epsilon} & \frac{\epsilon+1}{18\epsilon}
\end{bmatrix}$
\end{center}

Con este resultado es sencillo notar que el único $\epsilon$ que genera que A no tenga inversa es 0. Para el resto, A tiene inversa y por lo tanto el sistema de ecuaciones tiene una única solución. Cuando multiplicamos a $A^{-1}$ por b, obtenemos que el vector solución es x$^t$ = [1, 1, 1], independientemente del $\epsilon$. Por lo tanto, la solución del algoritmo de pivoteo parcial debería devolver siempre x$^t$ = [1, 1, 1]. Sin embargo, se espera que por error numérico esto no sea así. 

La norma infinito de $A$ es 6 y la de $A^{-1}$ es $\frac{\epsilon+16}{18\epsilon}$. Por lo tanto el numero de condición para $A$ es $\frac{\epsilon+16}{3\epsilon}$. Si $\epsilon$ tiende a cero, el numero de condición para $A$ es infinito. Si en cambio, tiende a infinito, el numero de condición es 1/3. Entonces, cuanto más grande es $\epsilon$, más chico es el numero de condición y más estable es A. 

El gráfico a continuación muestra la comparación entre el error absoluto vs $\epsilon$ para variables de 32 bits y 64 bits:

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.50]{./img/error_numerico_32vs64.png}}
\caption{Error Numérico}
\label{result_errorNumerico}
\end{figure}

Como podemos observar, el error numérico se encuentra delimitado con tope inferior y superior para cada $\epsilon$ y a menores $\epsilon$ el error numérico en la solución es mas grande y decrece a medida que aumenta $\epsilon$. De la misma forma, el error numérico para 64 bits es varios ordenes de magnitud más chico que para 32 bits, ya que se aumenta la precisión en los cálculos.

\subsection{Resultado Verificación de la implementación}
\label{resultados derivada}
Realizamos el cálculo de la derivada segunda de manera discreta para las tres funciones especificadas por la cátedra utilizando la implementación de eliminación Gaussiana para un sistema tridiagonal.
Se generaron los vectores d$\_vect\_a$, d$\_vec\_b$  y d$\_vec\_c$ que representan las funciones que se encuentran en la sección \ref{tridiagonal} y se calcularon las segundas derivadas para los tres casos.
Con estos resultados, se creó un único gráfico que se muestra en la siguiente Figura \ref{result_laplaciano} obteniendo el resultado esperado.

\begin{figure}[H]
\centerline{\includegraphics[scale=0.45]{./img/resultado_tridiag}}
\caption{Resultado de las funciones}
\label{result_laplaciano}
\end{figure}

Analizando con mayor profundidad cada función:\par
\begin{enumerate}
    \item[a)] La derivada segunda de la función (a) es 0 para todo x distinto a $i = n/2 + 1$. Luego, la derivada primera de la función (a) es constante para todo x $\not =$ i,lo que implica que la función es lineal para todo x $\not =$ i. Por lo que i es un punto de inflexión. Como la derivada segunda en i es positiva, la función tiene que ser cóncava para los positivos. Esta función descripta es coherente con la figura graficada.

    \item[b)] La derivada segunda de la función (b) es $\frac{4}{n^2}$ para todo x. Entonces, la derivada primera es lineal para todo x, lo que implica que la función es cuadrática para todo x. La función graficada se corresponde con una cuadrática.

    \item[c)] La derivada segunda de la función (c) es una función lineal para todo x. Esto significa que la derivada primera es cuadrática, implicando que la función debe ser un polinomio de grado tres (cúbica). La función graficada se corresponde con una cúbica.
\end{enumerate}

\subsection{Resultado Tiempos de cómputo}
\label{resultados tiempo}
\subsubsection{Ejercicio 5.1}
El objetivo de este ejercicio es obtener de forma experimental la complejidad temporal de los algoritmos utilizados en los puntos anteriores: eliminación Gaussiana con pivoteo, eliminación Gaussiana para matrices tridiagonales estándar y con precomputo. Las complejidades temporales teóricas de los algoritmos son:

\begin{itemize}
    \item Eliminación Gaussiana con pivoteo: $O(n^{3})$
    \item Eliminación Gaussiana para matrices tridiagonales estándar: $O(n)$
    \item Eliminación Gaussiana para matrices tridiagonales con precomputo: $O(n)$
\end{itemize}

El algoritmo de Eliminación Gaussiana con pivoteo tiene un triple for, necesario para la triangulación de la matriz A. Lo que genera que la complejidad temporal sea cúbica. En cambio los algoritmos de las matrices tridiagonales trabajan a nivel vectorial, modificando los valores de los vectores b y c. Lo que genera que la complejidad temporal sea lineal.

Para este ejercicio se definió \textit{A} como la matriz Laplaciana, es decir, aquella matriz que tiene a -2 como elementos de la diagonal y a 1 como los elementos de las dos subdiagonales.

\begin{figure}[H]
\centerline{\includegraphics[scale=0.45]{./img/tiempos_EGsinVsConPivoteo.png}}
\caption{Tiempo de computo para EG con pivoteo y EG en un sistema tridiagonal}
\label{result_ej5}
\end{figure}

\subsubsection{Ejercicio 5.2}

Con el resultado obtenido, se ve claramente el beneficio en el tiempo de ejecución al utilizar la segunda variante del algoritmo, especialmente cuando la cantidad de sistemas a resolver es crecientemente superior a la dimensión del sistema.

\begin{figure}[H]
\centerline{\includegraphics[scale=0.45]{./img/tiempos_tridiagConVsSinP.png}}
\caption{Tiempo de computo para Eliminación Gaussiana tridiagonal estandar vs. precomputada}
\label{result_ej5_2do}
\end{figure}


\subsection{Resultado Difusión}
\label{difusion}
\subsubsection{Ejercicio 6.1}
Dada la ecuación:

\begin{equation}
u_{i}^{(k)} - u_{i}^{(k-1)} = \alpha \times (u_{i-1}^{(k)} - 2u_{i}^{(k)} + u_{i+1}^{(k)})
\end{equation}

Se observa que la siguiente ecuación es la ecuación anterior expresada en forma matricial:

\begin{equation}
(I - \alpha \times H) \times u^{(k)} = u^{(k-1)}
\end{equation}

donde:

\begin{itemize}
  \item u$^{(j)}$: vector u en el paso j de difusión
  \item H: matriz Laplaciana de n $\times$ n
  \item I: matriz Identidad de n $\times$ n
\end{itemize}

Demostración:

\begin{equation}
u_{i}^{(k)} - u_{i}^{(k-1)} = \alpha \times (u_{i-1}^{(k)} - 2u_{i}^{(k)} + u_{i+1}^{(k)}) \space \forall i \in [1, n]
\end{equation}

\begin{equation}
\begin{bmatrix}
u_{1}\\
...\\
u_{n}
\end{bmatrix}^{(k)}
-
\begin{bmatrix}
u_{1}\\
...\\
u_{n}
\end{bmatrix}^{(k-1)}
=
\alpha \times H \times
\begin{bmatrix}
u_{1}\\
...\\
u_{n}
\end{bmatrix}^{(k)}
\end{equation}

\begin{equation}
u^{(k)} - u^{(k-1)} = \alpha \times H \times u^{(k)}
\end{equation}

\begin{equation}
u^{(k)} - \alpha \times H \times u^{(k)} =  u^{(k-1)}
\end{equation}

\begin{equation}
(I - \alpha H) \times u^{(k)} = A \times u^{(k)} =  u^{(k-1)}
\end{equation}

Consecuentemente, A = I - $\alpha \times$ H.

\subsubsection{Ejercicio 6.2}
Para investigar los distintos grados de difusión, ajustamos el parámetro $\alpha$, el cual se considera como una medida de la tasa de difusión en un modelo específico. Este valor representa una fracción del operador laplaciano, el cual interviene directamente en el cálculo de la difusión de un paso discreto al siguiente.
Al modificar $\alpha$, se observa cómo varía el patrón y la velocidad de difusión de la entidad en cuestión. Por ejemplo, la primer experimentación que se realizo se tomó $\alpha$ = 1 obteniendo el resultado que se muestra en la figura \ref{result_dif}. Se pudo observar que el gráfico obtenido es equivalente al brindado por la cátedra.

A partir de esto, se planteo que a partir de valores más altos de $\alpha$, la difusión sería mayor, siendo probable que veamos una difusión más rápida, donde la entidad se propaga más lejos desde su punto de origen en un período de tiempo determinado.
Mientras que con valores más bajos de $\alpha$, la difusión sería menor siendo probable que la difusión sea más lenta y limitada en alcance.
Para realizar al experimentación, se crearon los vectores con los mismos datos para $\alpha$ = 1, variando únicamente este ultimo parámetro al cual se le asigno valores de 0.1, 0.5 y 2.
A continuación, se generaron los cuatro gráficos que se pueden observar en las figuras \ref{fig:awesome_image1}, \ref{dif0.5} y \ref{fig:awesome_image2}.
%\begin{figure}[htbp]
%\centerline{\includegraphics[scale=0.45]{./img/result_dif.png}}
%\caption{Resultado difusión para $\alpha$ = 1}
%\label{result_dif}
%\end{figure}

\begin{figure}[!htb] 
  \label{ fig7} 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/alfa1.png}
  \caption{Difusión $\alpha$ = 0.1}\label{fig:awesome_image1} 
    \vspace{4ex}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
      \centering
    \includegraphics[width=.5\linewidth]{./img/alfa05.png} 
    \caption{Difusión $\alpha$ = 0.5} \label{dif0.5} 
    \vspace{4ex}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/alfa2.png}
  \caption{Difusion $\alpha$ = 2}\label{fig:awesome_image2}
    \vspace{4ex}
  \end{minipage}%% 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/result_dif.png}
  \caption{Difusión $\alpha$ = 1}\label{result_dif}
    \vspace{4ex}
  \end{minipage} 
\end{figure}

 \subsection{Resultado Difusión 2D}
\label{difusion2D}
\subsubsection{Ejercicio 7.1}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.45]{./img/2Dcasoborde0.png}}
\caption{Resultado simulación de difusión de calor en una placa 2D, condiciones de borde con temperatura 0}
\label{result_dif}
\end{figure}

\subsubsection{Ejercicio 7.2}
Realizando la comparación de los tiempos de cómputo para la resolución del sistema de ecuaciones de difusión de calor en 2D utilizando los dos métodos vistos anteriormente (eliminación gaussiana con y sin pivoteo) pudimos observar que, como era de esperar, el tiempo de cómputo aumenta de manera rápida a medida que el tamaño de la matriz crece. Esto se relaciona con el número de operaciones realizadas para resolver el sist. de ec. lineales ya que ésta crece aproximadamente de manera cúbica con el tamaño de la matriz.
Para matrices pequeñas (hasta tamaño 10x10), el tiempo de cómputo es prácticamente el mismo por lo que sugiere que el pivote no añade un costo computacional significativo. Sin embargo, para matrices más grandes (12x12 en adelante), se empieza a notar una diferencia pues el pivoteo mejora la estabilidad numérica del sistema.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.45]{./img/Dif2D_tiempos.png}}
\caption{ Tiempos de cómputo para la resolución del sistema de ecuaciones de difusión de calor en 2D}
\label{tiempos_dif2D}
\end{figure}

\subsubsection{Ejercicio 7.3}

\begin{figure}[!htb] 
  \label{ fig7} 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/instante0.png}
  \caption{Difusión $\alpha$ = 0.1}\label{instante0} 
    \vspace{4ex}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
      \centering
    \includegraphics[width=.5\linewidth]{./img/instante10.png} 
    \caption{Instante $t$ = 10} \label{instante10} 
    \vspace{4ex}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/instante50.png}
  \caption{Instante $t$ = 50}\label{instante50}
    \vspace{4ex}
  \end{minipage}%% 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=.5\linewidth]{./img/instante100.png}
  \caption{Instante $t$ = 100}\label{instante100}
    \vspace{4ex}
  \end{minipage} 
\end{figure}

\subsubsection{Ejercicio 7.4}
Dado que los bordes tienen una temperatura fija de 0, el calor tenderá a disiparse hacia estos, lo que limitará su propagación.
Podemos observar que en el instante t=10, el calor ya comenzó a propagarse significativamente desde el centro, pero todavía no ha llegado a los bordes de la placa, lo que es esperable porque aún estamos en un tiempo relativamente temprano en la simulación. A su vez, la distribución suave de la temperatura en los alrededores de la fuente central sugiere que el método implícito utilizado está funcionando bien, ya que evita oscilaciones o comportamientos inestables.

 \subsection{Opcionales}

 Como mencionamos en la sección \ref{opcionales}, el objetivo es calcular la matriz inversa de una matriz A utilizando el método de eliminación gaussiana (EG) con una matriz aumentada. Para ello se implemento la función \textit{invertir\_sistema} cuyo pseudocódigo es el siguiente: 
 
 \begin{algorithm}
\caption{Ejercicio opcional}\label{EG_SinP}
\begin{algorithmic}
\State \textbf{EG}(\textbf{in} A : matrix) $\to \textbf{A\_inv}$
 
 \State $n \gets A.shape[0]$
 \State $I \gets matriz\_identidad$
 \State $new\_A \gets eg(A,I)$
 \State $base\_matrix \gets new\_A[0:,:n]$
 \For{$i \gets n$ to $2*n$}
        \State  b $\gets$ new\_A.(1,n)
        \State x $\gets$ resolver$\_$sistema(base$\_$matrix,b)
        \State A$\_$inv[0:,i-n] $\gets$ x
\EndFor
\Return{$A\_inv$}
\end{algorithmic}
\end{algorithm}

Para resolver el problema utilizamos las funciones vistas en el trabajo practico.